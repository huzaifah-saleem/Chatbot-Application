# MCP Server
MCP_SERVER_URL=http://127.0.0.1:8001
MCP_ENDPOINT=/mcp

# LLM Provider: "local_llm", "gemini", "openai", or "nvidia_nim"
LLM_PROVIDER=nvidia_nim

# Local LLM Configuration (used by Local LLM and NVIDIA NIM providers)
# For Local LLM (Ollama): Use native API endpoint (e.g., http://127.0.0.1:11434)
# For NVIDIA NIM: Use OpenAI-compatible endpoint (e.g., http://127.0.0.1:8000/v1)
LOCAL_LLM_URL=your_url
LOCAL_LLM_MODEL=model_name

# Google Gemini Configuration
GEMINI_API_KEY=your_api_key
GEMINI_MODEL=model_name

# OpenAI Configuration
OPENAI_API_KEY=your_api_key
OPENAI_MODEL=model_name
